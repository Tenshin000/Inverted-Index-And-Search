\section{Spark code}
The Spark-related Python code was implemented based on the functional patterns and structure demonstrated during the lectures. \textbf{Apache Spark} is an open-source, distributed data processing engine designed for fast in-memory analytics and large-scale workload orchestration. Spark doesn’t strictly use \textbf{MapReduce}, but it supports map and reduce operations but runs them within a more flexible \textbf{DAG} execution model rather than the rigid two-stage MapReduce paradigm.

\vspace{4mm}

Spark jobs were executed on YARN with event logging and the Spark History Server enabled. The configuration included Kryo serialization, dynamic allocation with 3 executors (2 cores and 3GB RAM each), and speculative execution to handle stragglers—ensuring effective resource utilization and monitoring.

\vspace{4mm}

\textbf{RDD\_inverted\_index\_search.py} constructs an inverted index using \texttt{RDDs}. It loads documents from HDFS via \texttt{wholeTextFiles}, generating \texttt{(path, content)} pairs. Text is tokenized by lowercasing, removing non-alphanumerics, and splitting into words, producing key-value pairs of the form: $ ((\textbf{word}, \textbf{docID}), 1) $

These are aggregated using \texttt{reduceByKey}, mapped to \texttt{(word, (docID, count))}, and grouped by key to produce sorted postings lists. Partitioning is adjusted dynamically (1 partition per 44MB) to ensure workload balance. The final output can be saved as tab-delimited text, JSON, or Parquet, generating a distributed inverted index. This is the format of the final output:

\[
\texttt{word \textbackslash t f.name1:count1 \textbackslash t f.name2:count2}
\]

However, this code exhibited suboptimal performance, which was below our expectations for Spark. Therefore, we developed a second version implemented using Spark \texttt{DataFrames}. Spark's \textbf{DataFrames} wrap RDDs with a schema and declarative API, letting Spark Catalyst optimizer and Tungsten execution engine apply column-level and query-plan optimizations for far better performance and memory use than raw RDDs. 

In the new \textbf{inverted\_index\_search.py}, it first loads each specified path into a unified DataFrame annotated with a filename column, gracefully skipping any unreadable files. It then applies a sequence of Spark SQL transformations: all non‐alphanumeric characters are stripped via \texttt{regexp\_replace}, text is lowercased and split on whitespace and each word is exploded into its own row. Empty tokens are filtered out to ensure data quality. So we have a more optimized \textbf{tokenization}. In the next phase, the code groups by word and filename to compute per‐document term frequencies, then concatenates these as filename:count strings. A second grouping by word collects and sorts the full postings list into an array, producing one row per unique term with its complete, ordered document list. Finally the output is either written as plain text (with words and tab‐separated postings) JSON, or Parquet. This approach leverages Spark’s built‐in DataFrame optimizations and avoids manual RDD manipulations while delivering a scalable inverted index. 



