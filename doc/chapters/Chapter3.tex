\section{Dataset}

\hypersetup{
	colorlinks=true,
	urlcolor=blue
}

We selected a 1583.5 MB corpus of 2685 plaintext files from \href{https://www.gutenberg.org/}{Project Gutenberg}, covering diverse fields including philosophy, science, theology, psychology, literature and other cultural subjects, to stress and test our indexers across a broad range of real-world texts. This variety tests the system against typical literary content as well as challenging patterns, mirroring real-world search engine demands on both natural language and specialized data. File sizes vary from 5 KB to 250 MB: most are under 1 MB, 329 fall between 1 MB and 7 MB and one extreme outlier ("Human\_Genome\_Project-Chromosome\_1.txt", 250 MB) contains raw nucleotide sequences. Including this genomic text deliberately exposes our inverted-index builder to vast, mostly unique tokens-mimicking workloads with high vocabulary cardinality and ensuring our system handles both common-word skew and near-unique string distributions. By including files ranging from kilobytes to megabytes, the dataset enables a rigorous evaluation of how indexing and search-query systems scale with input size. 


